{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuraton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Vanilla ELECTRA settings\\n'adam_bias_correction': False,\\n'schedule': 'original_linear',\\n'sampling': 'fp32_gumbel',\\n'electra_mask_style': True,\\n'gen_smooth_label': False,\\n'disc_smooth_label': False,\\n'size': 'small',\\n'datas': ['openwebtext'],\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = MyConfig({\n",
    "    'device': 'cuda:0',\n",
    "    'base_run_name': 'vanilla',  # run_name = {base_run_name}_{seed}\n",
    "    'seed': 11081,  # 11081 36 1188 76 1 4 4649 7 # None/False to randomly choose seed from [0,999999]\n",
    "\n",
    "    'adam_bias_correction': False,\n",
    "    'schedule': 'original_linear',\n",
    "    'sampling': 'fp32_gumbel',\n",
    "    'electra_mask_style': True,\n",
    "    'gen_smooth_label': False,\n",
    "    'disc_smooth_label': False,\n",
    "\n",
    "    'size': 'small',\n",
    "#     'datas': ['openwebtext'],\n",
    "    'datas': ['wikipedia'],\n",
    "    'logger': \"wandb\",\n",
    "    'num_workers': 3,\n",
    "})\n",
    "\n",
    "\n",
    "\"\"\" Vanilla ELECTRA settings\n",
    "'adam_bias_correction': False,\n",
    "'schedule': 'original_linear',\n",
    "'sampling': 'fp32_gumbel',\n",
    "'electra_mask_style': True,\n",
    "'gen_smooth_label': False,\n",
    "'disc_smooth_label': False,\n",
    "'size': 'small',\n",
    "'datas': ['openwebtext'],\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process id: 7314\n",
      "{'device': 'cuda:0', 'base_run_name': 'vanilla', 'seed': 11081, 'adam_bias_correction': False, 'schedule': 'original_linear', 'sampling': 'fp32_gumbel', 'electra_mask_style': True, 'gen_smooth_label': False, 'disc_smooth_label': False, 'size': 'small', 'datas': ['wikipedia'], 'logger': 'wandb', 'num_workers': 3, 'run_name': 'vanilla_11081', 'mask_prob': 0.15, 'lr': 0.0005, 'bs': 128, 'steps': 1000000, 'max_length': 128}\n"
     ]
    }
   ],
   "source": [
    "# Check and Default\n",
    "assert c.sampling in ['fp32_gumbel', 'fp16_gumbel', 'multinomial']\n",
    "assert c.schedule in ['original_linear', 'separate_linear', 'one_cycle', 'adjusted_one_cycle']\n",
    "for data in c.datas:\n",
    "    assert data in ['wikipedia', 'bookcorpus', 'openwebtext']\n",
    "assert c.logger in ['wandb', 'neptune', None, False]\n",
    "\n",
    "if not c.base_run_name:\n",
    "    c.base_run_name = str(datetime.now(timezone(timedelta(hours=+8))))[6:-13].replace(' ','').replace(':','').replace('-','')\n",
    "if not c.seed:\n",
    "    c.seed = random.randint(0, 999999)\n",
    "\n",
    "c.run_name = f'{c.base_run_name}_{c.seed}'\n",
    "\n",
    "if c.gen_smooth_label is True:\n",
    "    c.gen_smooth_label = 0.1\n",
    "if c.disc_smooth_label is True:\n",
    "    c.disc_smooth_label = 0.1\n",
    "\n",
    "# Setting of different sizes\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "generator_size_divisor = [4, 3, 4][i]\n",
    "\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "# note that public electra-small model is actually small++ and don't scale down generator size \n",
    "gen_config.hidden_size = int(disc_config.hidden_size/generator_size_divisor)\n",
    "gen_config.num_attention_heads = disc_config.num_attention_heads//generator_size_divisor\n",
    "gen_config.intermediate_size = disc_config.intermediate_size//generator_size_divisor\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "\n",
    "# Path to data\n",
    "Path('./datasets').mkdir(exist_ok=True)\n",
    "Path('./checkpoints/pretrain').mkdir(exist_ok=True, parents=True)\n",
    "edl_cache_dir = Path(\"./datasets/electra_dataloader\")\n",
    "edl_cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Print info\n",
    "print(f\"process id: {os.getpid()}\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='google/electra-small-generator', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load/download wiki dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (./datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load/create data from wiki dataset for ELECTRA\n",
      "cache_file_name 1000_electra_wiki_128.arrow\n",
      "cache_file_name /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00000_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00001_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00002_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00003_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00004_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00005_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00006_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00007_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00008_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00009_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00010_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00011_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00012_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00013_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00014_of_00016.arrow\n",
      "Loading cached processed dataset at /home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00015_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from download_datasets import download_dset\n",
    "\n",
    "train_dset = download_dset(c, hf_tokenizer, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dset Dataset({\n",
      "    features: ['first_segment', 'input_ids', 'second_segment', 'sentA_length'],\n",
      "    num_rows: 20344249\n",
      "})\n",
      "HF_Dataset\n",
      "cols {'input_ids': <class 'fastai.text.data.TensorText'>, 'sentA_length': <function noop at 0x7f43eb1a0160>}\n",
      "n_inp 2\n",
      "MySortedDL\n",
      "pad_idx 0\n",
      "pad_idxs [0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miyamonz/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from get_dataloaders import get_dataloader\n",
    "dl = get_dataloader(c, hf_tokenizer, train_dset)\n",
    "\n",
    "from fastai.text.all import DataLoaders\n",
    "dls = DataLoaders(dl, path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158940"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00000_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00001_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00002_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00003_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00004_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00005_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00006_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00007_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00008_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00009_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00010_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00011_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00012_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00013_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00014_of_00016.arrow'},\n",
       " {'filename': '/home/miyamonz/ghq/github.com/miyamonz/electra_pytorch/pretrain/datasets/wikipedia/20200501.en/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63/1000_electra_wiki_128_00015_of_00016.arrow'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset.cache_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked language model objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLM objective callback"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from masked_lm_cb import MaskedLMCallback"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "mlm_cb = MaskedLMCallback(mask_tok_id=hf_tokenizer.mask_token_id,\n",
    "                          special_tok_ids=hf_tokenizer.all_special_ids,\n",
    "                          vocab_size=hf_tokenizer.vocab_size,\n",
    "                          mlm_probability=c.mask_prob,\n",
    "                          replace_prob=0.0 if c.electra_mask_style else 0.1,\n",
    "                          orginal_prob=0.15 if c.electra_mask_style else 0.1,\n",
    "                          for_electra=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA (replaced token detection objective)\n",
    "see details in paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ELECTRAModel, ELECTRALoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4367716990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed & PyTorch benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dls[0].rng = random.Random(c.seed) # for fastai dataloader\n",
    "random.seed(c.seed)\n",
    "np.random.seed(c.seed)\n",
    "torch.manual_seed(c.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator\n",
    "generator = ElectraForMaskedLM(gen_config)\n",
    "discriminator = ElectraForPreTraining(disc_config)\n",
    "discriminator.electra.embeddings = generator.electra.embeddings\n",
    "generator.generator_lm_head.weight = generator.electra.embeddings.word_embeddings.weight\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ELECTRA training loop\n",
    "electra_model = ELECTRAModel(generator, discriminator, hf_tokenizer, sampling=c.sampling)\n",
    "electra_loss_func = ELECTRALoss(gen_label_smooth=c.gen_smooth_label, disc_label_smooth=c.disc_smooth_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from pl_model import LitElectra\n",
    "model = LitElectra(generator, discriminator, hf_tokenizer, sampling=c.sampling, config=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                  | Params\n",
      "--------------------------------------------------------\n",
      "0 | generator     | ElectraForMaskedLM    | 4.6 M \n",
      "1 | discriminator | ElectraForPreTraining | 13.5 M\n",
      "--------------------------------------------------------\n",
      "14.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.2 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8ab23b96c949fc8c5b96695f06b5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LambdaLR' object has no attribute 'param_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-584000b472a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_fit_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;31m# ------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# when returning -1 from train_step, we end epoch early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;31m# toggle model params + set info to logger_connector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train_split_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_train_split_start\u001b[0;34m(self, split_idx, split_batch, opt_idx, optimizer)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoggle_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;31m# use to track metrics internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/electra-pytorch-3wiafTge-py3.8/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36mtoggle_optimizer\u001b[0;34m(self, optimizer, optimizer_idx)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0mparam_requires_grad_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_pl_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                     \u001b[0;31m# If a param already appear in param_requires_grad_state, continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LambdaLR' object has no attribute 'param_groups'"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "trainer.fit(model, dl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from fastai.text.all import Learner\n",
    "from _utils.would_like_to_pr import RunSteps\n",
    "# Learner\n",
    "dls.to(torch.device(c.device))\n",
    "learn = Learner(dls, electra_model,\n",
    "                loss_func=electra_loss_func,\n",
    "                opt_func=opt_func,\n",
    "                path='./checkpoints',\n",
    "                model_dir='pretrain',\n",
    "                cbs=[mlm_cb,\n",
    "                    RunSteps(c.steps, [0.0625, 0.125, 0.25, 0.5, 1.0], c.run_name+\"_{percent}\"),\n",
    "                    ],\n",
    "                )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from _utils.would_like_to_pr import GradientClipping\n",
    "# Mixed precison and Gradient clip\n",
    "learn.to_native_fp16(init_scale=2.**11)\n",
    "learn.add_cb(GradientClipping(1.))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit(1, cbs=[lr_shedule])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print time and run name\n",
    "print(f\"{c.run_name} , starts at {datetime.now()}\")\n",
    "\n",
    "# Run\n",
    "if c.schedule == 'one_cycle':\n",
    "    learn.fit_one_cycle(9999, lr_max=c.lr)\n",
    "elif c.schedule == 'adjusted_one_cycle':\n",
    "    learn.fit_one_cycle(9999, lr_max=c.lr, div=1e5, pct_start=10000/c.steps)\n",
    "else:\n",
    "    learn.fit(9999, cbs=[lr_shedule])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
