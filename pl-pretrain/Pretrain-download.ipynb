{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuraton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = MyConfig({\n",
    "    'device': 'cuda:0',\n",
    "    'base_run_name': 'vanilla',  # run_name = {base_run_name}_{seed}\n",
    "    'seed': 11081,  # 11081 36 1188 76 1 4 4649 7 # None/False to randomly choose seed from [0,999999]\n",
    "\n",
    "    'adam_bias_correction': False,\n",
    "    'schedule': 'original_linear',\n",
    "    'sampling': 'fp32_gumbel',\n",
    "    'electra_mask_style': True,\n",
    "    'gen_smooth_label': False,\n",
    "    'disc_smooth_label': False,\n",
    "\n",
    "    'size': 'small',\n",
    "#     'datas': ['openwebtext'],\n",
    "    'datas': ['wikipedia'],\n",
    "    'logger': \"wandb\",\n",
    "    'num_workers': 3,\n",
    "})\n",
    "\n",
    "\n",
    "\"\"\" Vanilla ELECTRA settings\n",
    "'adam_bias_correction': False,\n",
    "'schedule': 'original_linear',\n",
    "'sampling': 'fp32_gumbel',\n",
    "'electra_mask_style': True,\n",
    "'gen_smooth_label': False,\n",
    "'disc_smooth_label': False,\n",
    "'size': 'small',\n",
    "'datas': ['openwebtext'],\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check and Default\n",
    "assert c.sampling in ['fp32_gumbel', 'fp16_gumbel', 'multinomial']\n",
    "assert c.schedule in ['original_linear', 'separate_linear', 'one_cycle', 'adjusted_one_cycle']\n",
    "for data in c.datas:\n",
    "    assert data in ['wikipedia', 'bookcorpus', 'openwebtext']\n",
    "assert c.logger in ['wandb', 'neptune', None, False]\n",
    "\n",
    "if not c.base_run_name:\n",
    "    c.base_run_name = str(datetime.now(timezone(timedelta(hours=+8))))[6:-13].replace(' ','').replace(':','').replace('-','')\n",
    "if not c.seed:\n",
    "    c.seed = random.randint(0, 999999)\n",
    "\n",
    "c.run_name = f'{c.base_run_name}_{c.seed}'\n",
    "\n",
    "if c.gen_smooth_label is True:\n",
    "    c.gen_smooth_label = 0.1\n",
    "if c.disc_smooth_label is True:\n",
    "    c.disc_smooth_label = 0.1\n",
    "\n",
    "# Setting of different sizes\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "generator_size_divisor = [4, 3, 4][i]\n",
    "\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "# note that public electra-small model is actually small++ and don't scale down generator size \n",
    "gen_config.hidden_size = int(disc_config.hidden_size/generator_size_divisor)\n",
    "gen_config.num_attention_heads = disc_config.num_attention_heads//generator_size_divisor\n",
    "gen_config.intermediate_size = disc_config.intermediate_size//generator_size_divisor\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "\n",
    "# Path to data\n",
    "Path('./datasets').mkdir(exist_ok=True)\n",
    "Path('./checkpoints/pretrain').mkdir(exist_ok=True, parents=True)\n",
    "edl_cache_dir = Path(\"./datasets/electra_dataloader\")\n",
    "edl_cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Print info\n",
    "print(f\"process id: {os.getpid()}\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from download_datasets import download_dset\n",
    "\n",
    "train_dset = download_dset(c, hf_tokenizer, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from get_dataloaders import get_dataloaders\n",
    "\n",
    "dls = get_dataloaders(c, hf_tokenizer, train_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset.cache_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
